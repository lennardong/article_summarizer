{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "# NLP\n",
    "import spacy \n",
    "\n",
    "# Utils \n",
    "import os \n",
    "import random\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc File: 382\n",
      "Title Sample: Ban on forced retirement under 65\n",
      "Triple Sample: ['employer | justify | force', 'employer | have | right', 'people | forced | work_long_than_people_want', 'business_leader | oppose | age_discrimination_proposal', 'British_Chambers | welcome | proposal', 'employer | have | ability', 'people | mount | challenge', 'worker | collect | worker_state_pension']\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "##########################################################################################################################################\n",
    "\n",
    "def load_articles(PATH):\n",
    "    \"\"\"\n",
    "    Returns a list of strings, where each string is an article.\n",
    "    Order is sorted by article name\n",
    "    \"\"\"\n",
    "\n",
    "    news_files = sorted([f for f in os.listdir(PATH) if f.endswith('.txt')])\n",
    "    \n",
    "\n",
    "    news_list = []\n",
    "    for file in news_files:\n",
    "        with open(os.path.join(PATH, file), 'r') as f:\n",
    "            text = f.read()\n",
    "            news_list.append(text)\n",
    "    \n",
    "    return news_list\n",
    "\n",
    "def load_triples(PATH):\n",
    "    \"\"\"\n",
    "    Returns a nested list of triples by filename and triple\n",
    "    Order is sorted by file name.\n",
    "    \"\"\"\n",
    "    triple_files = sorted([f for f in os.listdir(PATH) if f.endswith('.txt')])\n",
    "    # print(triple_files)\n",
    "\n",
    "    triple_arr = []\n",
    "    for file in triple_files:\n",
    "        with open(os.path.join(PATH, file), 'r') as f:\n",
    "            t = []\n",
    "            lines = f.read().strip().split('\\n')\n",
    "            t.extend(lines)\n",
    "        triple_arr.append(t)\n",
    "\n",
    "    return triple_arr\n",
    "\n",
    "\n",
    "def seperate_title_and_body(article, VERBOSE = False): \n",
    "    \"\"\"\n",
    "    Input: a string file\n",
    "    Returns: list of format (title, [para1, para2, para3, ...])\n",
    "    \"\"\"\n",
    "\n",
    "    paragraphs = article.split(\"\\n\\n\")\n",
    "\n",
    "    title = paragraphs[0]\n",
    "    body = ' '.join(paragraphs[1:])\n",
    "    if VERBOSE: \n",
    "        print(\"title:\", title)\n",
    "        print(\"body:\", body)\n",
    "    \n",
    "    return (title, body)\n",
    "\n",
    "\n",
    "##########################################################################################################################################\n",
    "# EXECUTION\n",
    "##########################################################################################################################################\n",
    "\n",
    "FOLDER_TRIPLES = \"data/BBC/Training_strict/business_triples\"\n",
    "FOLDER_RAW = \"./data/BBC/News Articles/business\"\n",
    "\n",
    "# Load Triples and Raw Articles\n",
    "all_triples = load_triples(FOLDER_TRIPLES)\n",
    "articles = load_articles(FOLDER_RAW)\n",
    "\n",
    "# Generate list of titles\n",
    "titles = []\n",
    "for article in articles:\n",
    "    title, body = seperate_title_and_body(article, VERBOSE = False)\n",
    "    titles.append(title)\n",
    "\n",
    "# sanity Check\n",
    "assert (len(all_triples) == len(titles) == len(articles)), \"Counts do not match. (triples vs titles vs articles)\"\n",
    "\n",
    "id = random.randrange(0, len(titles))\n",
    "print(f\"Doc File: {id + 1}\")\n",
    "print(f\"Title Sample: {titles[id]}\")\n",
    "print(f\"Triple Sample: {all_triples[id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========Input Sys Prompt============\n",
      "You are a triplet-to-paragraph generator.\n",
      "\n",
      "#Brief: \n",
      "Anything between [] is the task inputs.\n",
      "# indicates the article title \n",
      "\"sub | veb | obj\" describes a series of verb, object triples related to title. \n",
      "\n",
      "#Task:\n",
      "The task is to generate a short summary paragraph with the title at top. \n",
      "It should be factually based only on the triples. \n",
      "Inferences should only be made between the triples and the title. \n",
      "Do not add embellishments. \n",
      "Do organize the paragraph so it has a logical flow.\n",
      "Keep it as simple and direct as possible. \n",
      "\n",
      "#Example Input:\n",
      "[#China now top trader with Japan\n",
      "\n",
      "china | overtook | us\n",
      "change | highlights | chinagrowing importance\n",
      "trade | was hurt | factors\n",
      "analysts | see | spurs\n",
      "Japan trade surplus | grew | trade\n",
      "Japan trade surplus | accounted | trade\n",
      "\n",
      "#Example Output:\n",
      "China now top trader with Japan\n",
      "\n",
      "China has overtaken the US as Japan's top trading partner. \n",
      "This change highlights China's growing importance in the region. \n",
      "Trade was hurt by various factors, but analysts see this as a spur to \n",
      "further growth. Japan's trade surplus grew as a result, with the surplus \n",
      "accounting for a significant portion of the trade.\n",
      "\n",
      "=========Input User Prompt============\n",
      "[#Consumer spending lifts US growth\n",
      "\n",
      "President_George_W_Bush | take | office\n",
      "US_Federal_Reserve | raise | interest_rate\n",
      "US_Federal_Reserve | take | Reserve_time\n",
      "challenger_John_Kerry | criticise | handling\n",
      "economy | regain | traction_Gary_Thayer\n",
      "policy | generate | growth\n",
      "rate | mark | increase\n",
      "rate | reflect | jump]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################################\n",
    "# Inputs\n",
    "##########################################################################################################################################\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a triplet-to-paragraph generator.\n",
    "\n",
    "#Brief: \n",
    "Anything between [] is the task inputs.\n",
    "# indicates the article title \n",
    "\"sub | veb | obj\" describes a series of verb, object triples related to title. \n",
    "\n",
    "#Task:\n",
    "The task is to generate a short summary paragraph with the title at top. \n",
    "It should be factually based only on the triples. \n",
    "Inferences should only be made between the triples and the title. \n",
    "Do not add embellishments. \n",
    "Do organize the paragraph so it has a logical flow.\n",
    "Keep it as simple and direct as possible. \n",
    "\n",
    "#Example Input:\n",
    "[#China now top trader with Japan\n",
    "\n",
    "china | overtook | us\n",
    "change | highlights | chinagrowing importance\n",
    "trade | was hurt | factors\n",
    "analysts | see | spurs\n",
    "Japan trade surplus | grew | trade\n",
    "Japan trade surplus | accounted | trade\n",
    "\n",
    "#Example Output:\n",
    "China now top trader with Japan\n",
    "\n",
    "China has overtaken the US as Japan's top trading partner. \n",
    "This change highlights China's growing importance in the region. \n",
    "Trade was hurt by various factors, but analysts see this as a spur to \n",
    "further growth. Japan's trade surplus grew as a result, with the surplus \n",
    "accounting for a significant portion of the trade.\"\"\"\n",
    "\n",
    "doc_prompts = []\n",
    "for idx in range(len(all_triples)):\n",
    "    triples = sorted(all_triples[idx])\n",
    "    title = titles[idx]\n",
    "    body = '\\n'.join(triples)\n",
    "    formatted_text = \"[#\" + title + \"\\n\\n\" + body + \"]\"\n",
    "    # prompts.append(PROMPT + \"\\n\\n==============\\n\\n\" + formatted_text)\n",
    "    doc_prompts.append(formatted_text)\n",
    "\n",
    "print(f\"\\n=========Input Sys Prompt============\\n{SYS_PROMPT}\")\n",
    "print(f\"\\n=========Input User Prompt============\\n{doc_prompts[random.randrange(0, len(doc_prompts))]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m##########################################################################################################################################\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# TESTING\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m##########################################################################################################################################\u001b[39;00m\n\u001b[0;32m     24\u001b[0m prompt \u001b[38;5;241m=\u001b[39m doc_prompts[random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(doc_prompts))] \n\u001b[1;32m---> 25\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mquery_gpt3_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSYS_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmodel_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mquery_gpt3_chat\u001b[1;34m(sys_prompt, user_prompt, model_engine)\u001b[0m\n\u001b[0;32m      6\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sys_prompt},\n\u001b[0;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_prompt},\n\u001b[0;32m     11\u001b[0m ]\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__prepare_create_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    104\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 106\u001b[0m requestor \u001b[38;5;241m=\u001b[39m \u001b[43mapi_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIRequestor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    115\u001b[0m     deployment_id,\n\u001b[0;32m    116\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     params,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\openai\\api_requestor.py:130\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     organization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    128\u001b[0m ):\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m api_base \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    132\u001b[0m         ApiType\u001b[38;5;241m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m api_type\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m ApiType\u001b[38;5;241m.\u001b[39mfrom_str(openai\u001b[38;5;241m.\u001b[39mapi_type)\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_version \u001b[38;5;241m=\u001b[39m api_version \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_version\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key provided. You can set your API key in code using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key = <API-KEY>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key_path = <PATH>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
     ]
    }
   ],
   "source": [
    "##########################################################################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "##########################################################################################################################################\n",
    "\n",
    "def query_gpt3_chat(sys_prompt, user_prompt, model_engine=\"gpt-3.5-turbo\"):\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_engine,\n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "##########################################################################################################################################\n",
    "# TESTING\n",
    "##########################################################################################################################################\n",
    "\n",
    "prompt = doc_prompts[random.randrange(0, len(doc_prompts))] \n",
    "generated_text = query_gpt3_chat(sys_prompt= SYS_PROMPT, \n",
    "                                 user_prompt = prompt, \n",
    "                                 model_engine=\"gpt-3.5-turbo\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################\n",
    "# Generate Summaries\n",
    "##########################################################################################################################################\n",
    "import time\n",
    "\n",
    "PAUSE = 10\n",
    "PAUSE_ON_ERROR = 60\n",
    "FOLDER = \"./results/Generated_ChatGPTPrompt/\"\n",
    "if not os.path.exists(FOLDER):\n",
    "    os.makedirs(FOLDER)\n",
    "\n",
    "for idx, doc_prompt in tqdm(enumerate(doc_prompts)):\n",
    "    \n",
    "    # For error recovery\n",
    "    # problems: 057, 067, 154\n",
    "    if idx < 262: continue\n",
    "\n",
    "    # Generate text\n",
    "    try: \n",
    "        generated_text = query_gpt3_chat(sys_prompt= SYS_PROMPT, \n",
    "                                    user_prompt = doc_prompt, \n",
    "                                    model_engine=\"gpt-3.5-turbo\")\n",
    "    except:\n",
    "        time.sleep(PAUSE_ON_ERROR)\n",
    "        generated_text = query_gpt3_chat(sys_prompt= SYS_PROMPT, \n",
    "                                    user_prompt = doc_prompt, \n",
    "                                    model_engine=\"gpt-3.5-turbo\")\n",
    "        print(f\"Paused and resolved on idx: {idx}\")\n",
    "     \n",
    "    # Save summary as txt file\n",
    "    filename = f\"{idx + 1:03d}.txt\"\n",
    "    path = os.path.join(FOLDER, filename)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(generated_text)\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(PAUSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
